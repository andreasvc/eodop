\documentclass[10pt,a4paper]{article}
%\usepackage{linguex}
%\usepackage{fullpage}
%\usepackage{synttree}
\usepackage{multicol}
\usepackage{microtype}
\usepackage{mdwlist}
\usepackage{natbib}
\setlength{\bibsep}{0.0pt}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage[english]{babel}
\usepackage[colorlinks=true, linkcolor=black, urlcolor=blue, pdfborder={0 0 0}]{hyperref}
%\usepackage[utf8x]{inputenc}

% title page stuff
\input{titles}

\begin{document}

\title{Enriching Data-Oriented Parsing by blending morphology and syntax}

\subtitle{a pilot using an agglutinative constructed language}

\author{Andreas van Cranenburgh\footnote{acranenb@science.uva.nl, 0440949}}

\maketitle

\begin{center}
Cognitive Models of Language project, \\
Master of Logic, University of Amsterdam 
\end{center}

\vspace{5em}
\abstract{
Esperanto is a constructed language with a rich and regular morphology.  It
seems likely that taking its morphology into account when parsing syntax will
improve accuracy. We propose a way of investigating the effects of considering
morphological and phrase structure analysis as separate, autonomous steps,
versus combining them into a single \textsc{dop} model. We will assume a hierarchical
representation for both syntax and morphology, which is in contrast with most
work in natural language processing which is largely limited to finite-state
morphology.

To this end, we describe a formal grammar that enumerates the word forms of
Esperanto. Using a \textsc{dop} model words that have been segmented into sequences of
morphemes and tags can be analysed and assigned a hierarchical structure. The
resulting \textsc{dop} model can either be merged with a syntactic treebank into a
combined \textsc{dop} model, or mapped to the leaves of the parse trees produced by a
syntactic model, to obtain tree structures with both phrasal and morphological
constituents. 
%
%The resulting system will be applied to a small corpus of morphology and
%syntax. Evaluation with a larger syntactic treebank, as well as the induction
%of morphology tags from dictionaries remains to be done.
}\footnote{Acknowledgements: I wish to thank the following people (in reverse
chronological order): Federico Sangati for practical advice on \textsc{dop}, Ken Miner
for advice on morphology and suggesting the application of \textsc{dop} to Esperanto,
Eckhard Bick for the Monato treebank, Rens Bod for teaching me Data-Oriented
Parsing, and last but not least Wim Jansen for teaching me Esperanto.}

\newpage

\tableofcontents

%\begin{multicols}{2}

\section{Introduction}
\subsection{Research questions}

%\begin{quote}
\noindent
\emph{
Does integrating morphology with syntax improve parsing results for
syntax?}

\noindent
\emph{
Are morphology and syntax autonomous? i.e., is morphology opaque or
transparent to syntax?
}
%\end{quote}

These possibilities correspond to a modularist (cf., Pinker 1994, Jackendoff
2003) vs. an interactionist approach (cf., MacWhinney 1987). Here modularism
refers to the functionalist\footnote{Functionalism refers here to the position
in the philosophy of mind positing encapsulated mental modules; not to be
confused with the Functionalist school of linguistics.} hypothesis of the
autonomy of syntax from other levels, both the stronger claim of processing
autonomy (full modularity, encapsulation), and its weaker form of
representational autonomy. In effect the issue at stake here is the nature of
the morphology-syntax interface. On the one hand there is the extreme of syntax
seeing only a Part-of-Speech tag (and possibly the unanalyzed word or lemma as
well if syntax is lexicalised), on the other hand there is the other extreme
where morphology is literally a part of syntax that has been conveniently
ignored in the majority of work in (computational) linguistics to date.
Jackendoff's (2003) parallel architecture suggest a compromise (at least for
generativists) where interfaces of different autonomous levels are possible
(e.g., phonology-semantics to deal with focus effects).

%\begin{quote}
\noindent
\emph{
Are words the smallest units of syntax, or is it perhaps morphemes? A
fully transparent morphology could imply that morphemes are the smallest units
of syntax, rather than words as is customarily assumed.
}
%\end{quote}

%Naturally considering morphemes as units of syntax is most relevant in radically agglutinative and polysynthetic languages,
%and irrelevant for isolating languages.

Because this is a pilot project, I shall only attempt to answer the first
question, but this may provide a hint as to the other questions. In addition
the answer to the first question shall only concern Esperanto, which has been
chosen for its rich and regular morphology. Recent work by Tsarfaty (2010)
underscores the need for adapting formalisms to morphologically rich and
relatively free word-order languages such as Hebrew and Arabic, but also
Esperanto. Without morphology-aware parsers it does not seem possible to attain
the current parse accuracy of languages such as English and Chinese, which is 
dependent on their analyticity.

\subsection{Approach}

\begin{enumerate}

\item Construct a corpus of sentences annotated with phrase structures, and a 
lexicon of words annotated with morphological structures. The assumption is
that while syntax may use information in morphology, morphology does not need
information from syntax, hence the possibility of constructing a morphological
corpus independent of the text corpus; note that this amounts to assuming that
for the purpose of constructing a corpus the morphology is context-free.

\item Divide the corpus into training and testing, train on the former with \textsc{dop1} or 
\textsc{dop*}\footnote{More on Data-Oriented Parsing further on.} (Zollman 2005) %TODO TODO TODO

\item Measure performance of syntax model, this will be the baseline

\item Morphology transparent to syntax: take treebank corpus, merge phrase 
structure trees with morphological analyses, construct a single \textsc{dop} model

\item Morphology opaque to syntax: construct a \textsc{dop} model for morphology, taking
one word at a time, and a \textsc{dop} model for syntax, producing phrase structure
trees without morphology. Morphological structure and phrase structure can be
parsed in parallel and independent of each other.

\end{enumerate}

\section{Background}
\subsection{Morphology}

Most work in Computational Linguistics focuses exclusively on syntax; this is
a form of syntactocentrism, a term coined in Generative Linguistics 
(Jackendoff 2003). This also goes for Data-Oriented Parsing (\textsc{dop}), although
excursions into semantics have been made. In this project I will go in the
other direction and turn to the lower linguistic stratum of morphology. Most
accounts of morphology in Computational Linguistics seem to present the
structure of words as merely a sequence of morpheme-feature pairs (e.g.,
Jurafsky \& Martin 2000), as parsed by a Finite State Transducer (cf., Schmid
et al.  2004). During the course of this project I even came across a master
thesis describing such a two-level morphology for Esperanto (Hana 1998).

However, due to the complexity and potentially unlimited productivity of
morphology in Esperanto such a representation will necessarily reveal
only part of the structural information of words in Esperanto (more on this
in the next section). Such an approach is to the representation of the present
project what POS tagged sentences are to hierarchical phrase structure trees.
Although the present project focuses on Esperanto, the method of adding
morphology to \textsc{dop} should generalize to other languages, also to languages
such as English which display only a very limited amount of morphological
productivity and hence exhibit only a subset of the derivational complexity in
morphologically richer languages.

\subsection{About Esperanto}

Esperanto is a constructed language (also referred to as a planned language).
The term ``artificial language" that is sometimes employed is inappropriate, as
its artificial design is only a point in time of its century long continuous
usage and evolution. It is a spoken language with its own literature and
culture, so while it may not  be a ``natural language" strictly speaking (Gobbo
(2009) uses the term Quasi-Natural Language), it is certainly a human language
that performs all the communicative and expressive functions of ethnic
languages, albeit mostly as a second language used by a diverse and scattered
speech community. When Esperanto is referred to in the popular press as a
``failed project'' this refers to the ambitious pacifistic ideals of the
language, not its maturation as a langauge.

Typologically Esperanto has the unique character of being a morphologically
agglutinative and synthetic language, yet with a vocabulary largely based on
Romance languages (apart from some German \& Russian words, and schematic
function words as well). Its word formation is extremely compositional (i.e.,
complex words are semantically fully transparent); I would go as far as to
contend that it is the most compositional spoken language in use today. Its
syntax is schematic (designed) and allows for a relatively free word-order
through obligatory case marking, although in practice a default word-order of
SVO has emerged, with systematic deviations, triggered by complex (heavy)
constituents and by pragmatics to express focus; these findings accord with
relatively universal features found in natural languages (Jansen 2007). Cases
are marked, viz. through a null-marking for the nominative and indirect object,
an inflection in case of the accusative, and through a set of prepositions
initially intended to be unambiguous (e.g., the English preposition ``with"
translates in two ways in Esperanto, through the instrumentalis ``per" or with
``kun," meaning together). 

The qualification ``relatively'' is a commonly made one for the freeness of
word-order. To be specific, it refers here to the fact that within
constituents word-order is fixed for determiners, prepositions and negation
\& degree particles, while being free for adjectives and nouns. The order of
constituents has a higher amount of freedom, but the order of prepositional
phrases does reflect their place in argument structures. Lastly wh-question formation
co-occurs with a word-order transformation, i.e., if the wh-constituent is an
object it is moved to sentence initial position.  Strangely enough
this does not happen for polar questions, which are marked with a
polar question forming particle. This leaves Esperanto in the perhaps uncommon position
of marking one type of question with word-order (which is desirable since the
interrogative pronouns also serve as relative pronouns), and the other with a particle.

We should also consider the ambiguities introduced by relaxing word-order.
Since the accusative is marked through a declension with obligatory agreement,
it is trivial to distinguish subjects and objects. However, boundaries between
other constituents such as the nominative and the indirect object or the end of
prepositional phrases are unmarked, and can result in ambiguity due to the
aforementioned underspecification.

Concerning prepositions, the initial intention was to express some rather vague
relations such as ``believing {\em in} God" (which is neither spatial nor
temporal, it would appear) with a semantically neutral preposition for an
unspecified relation, the preposition ``je"; however, this seems to have fallen
in disuse, probably through interference from Ethnic languages. However, an
interesting hypothesis could be that this reflects an evolutionary pressure for
distinctions and ambiguities to correspond with the meanings that are actually
expressed (the prior probability of wanting to express some meaning) -- while
an abstruse philosophical treatise may theoretically discuss ``believing'' while
residing spatially or temporally ``in God,'' this possibility is vanishingly rare
so that making the distinction is wasted effort.

While Esperanto's morphology is agglutinative and synthetic,\footnote{Esperanto
has an index of agglutination of 1.0 and an average synthesis index
(word-morpheme ratio) of 1.8-2, reported by Wells 1989} it is not
poly-synthetic such as Inuit languages; single words cannot express what is
denoted by a whole phrase in other languages, and grammatical roles are not
marked, nor is the nature of the relation between elements that make up a word
specified. It also does not feature incorporation such as in Catalan.
Concerning the underspecification of relations between morphemes, consider the
Dutch word ``zoektechnieken", which could translated as ``techniques for search,"
though ``for" is not specified in the Dutch word. In an agglutinative language
invariant morphemes that express only a single grammatical meaning are
concatenated unmodified, such that identifying the elements that make up a word
is relatively easy\footnote{Ambiguities may arise through overlap; i.e., when
concatenating two smaller morphemes results in a string of characters that
coincides with a larger morpheme.}. The process of word formation is completely
productive and without exceptions; the only proviso is that a formation should
make sense semantically when considering the meaning of its constituent
elements (i.e., the principle of compositionality modulo the Gricean maxim of
manner). 

There is obligatory agreement in number and declension within noun phrases.
Verb paradigms are simple: tense is marked with the ending, person and number
solely through the subject.

\newpage

Esperanto's productive morphology can be summarized using a regular
grammar. The following is adapted from Schubert (1993)\footnote{Caveat lector:
Schubert incorrectly characterizes this grammar as recursive.}, which in turn is
based on Kalocsay's (1980) account. I have translated it into a regular
grammar, proving that the word forms in the lexicon of Esperanto can be
enumerated by a regular language; to my knowledge this is the first such
description to date.  The grammar for function words: 

\begin{verbatim}
function_word := adverb | preposition | numeral
adverb := prefix adverb
preposition := prefix preposition
numeral := numeral numeral*
prefix := mal | ne | ...
suffix := il | et | ...
\end{verbatim}

Content words are a little more involved (ibid):

%\begin{verbatim}
\noindent
\texttt{  \\
word := prefix* left* right ending ($\varepsilon$ | declension) \\
left := right ($\mathtt\varepsilon$ | ending) \\
right := prefix* root suffix* \\
ending := o | a | e \\
declension := j | n \\
verb-ending := as | is | os | us | u \\
root := akv | far | ...
}
%\end{verbatim}

In these rules, ``prefix" and ``suffix" refers to a closed class of affixes;
``(verb-)ending" refers to a one or two-character ending marking the
Part-of-Speech; ``declension" refers to either a null marking (nominative,
singular) or the accusative and/or plurality marking. Furthermore,
``\texttt{*}" is the Kleene star, ``\texttt{|}" is the alternation operator,
and lastly concatenation is implied. This grammar incorporates three processes
of word-formation in Esperanto: derivation (concatenating elements to form
words), compounding (concatenating elements to words to form more complex
words), and POS category change.  The latter refers to nominalizations and
other possible mappings between Parts-of-Speech.

While this grammar should in all likelihood exhaust Esperanto's morphology, 
it is of little use for computational linguistics because of its ambiguity
and flat structure.  Whereas POS-tagging can be done practically error-free
using a rule-based algorithm (save for proper names and foreign words), deeper
morphological structure will depend on the morphemes in question, and probably
their semantics as well. However, in this project it is assumed that the latter
does not play a major role as doing semantics is infeasible\footnote{It is my
contention that semantics relies on extensive extra-linguistic world
knowledge.}. We will assume that derivations and compound words are constructed
in a stochastic process that can be leared from examples (words with their
appropriate structure, that is).

Another way in which the grammar falls short is that it does not consider the
grammatical character of roots in Esperanto (Schubert 1993). Although initially
controversial, the thesis that bare roots (without their grammatical endings)
have a  grammatical category to which they belong has by now been almost
universally accepted in Esperantology. In effect this entails that roots in
Esperanto belong to a prototypical semantic class (sometimes several).  These
classes are verbal, adjectival and noun-like; adverbs are part of the
adjectival roots, arguably making up a ``qualities" class. The
typical example is ``\textsc{martel}" and ``\textsc{tond},'' roots for hammer and cutting,
respectively. The category of the former is a noun and thus ``martelo" means a
hammer, and the derived ``marteli" means to hammer. The latter is a verbal root, 
with "tondi" meaning to cut, and the derivation ``tondilo" meaning a
tool to cut or a scissor, requiring an affix to denote a tool derived from a
verb (directly affixing a noun ending to the root would mean ``a cut"). Without
recording the grammatical category of roots, a model of Esperanto morphology
would not be able to predict the correct derivations and curtail overgeneration.

The present work glosses over a related feature of Esperanto roots, the fact
that verbs are transitive or intransitive (valency), requiring an affix to
change from the one to the other meaning. The reason for glossing over this
aspect is that this information should become part of a more general account of
argument structure (i.e., including prepositional arguments) that is beyond the
scope of this project. Take these examples:

(1) ``La akvo bolas" (the water boils)

(2) ``Mi boligas la akvon" (I boil the water)

(3) ``Mi finis la libron" (I finished the book)

(4) ``La libro fini\^gis" (the book finished)

Sentences (1) and (3) contain the original verb, while (2) and (4) contain
affixed verbs with a different subcategorization frame. This feature of
Esperanto has been critized as being a needless distinction (common sense
usually yields the correct meaning, as for example English demonstrates),
as well as the rather arbitrary choices that have been made as to the
transitivity of certain verbs, requiring a language user to memorize them by
rote. It has also resulted in confusing paronyms such as ``pesi'' (to weigh
something) and ``pezi'' (to weigh $X$ kilos, to be heavy). It is however an
unchangeable part of the language.

Previous work with Esperanto has resulted in a highly successful
($>95\%$ precision on a small test corpus) constraint grammar (Bick 2007), and
a formal model of morphology and syntax in the form of an adpositional grammar
(Gobbo 2009); an adpositional grammar is a dependency grammar combining
directed dependencies with the dimension of trajector/landmark from
construction grammar. These provide a means of comparison and a potential
treebank.

Since there is no gold standard treebank with phrase structures for Esperanto,
I will construct a small toy corpus for testing, as well as exploring a
treebank generated with EspGram (Bick 2007) from a magazine corpus. Furthermore,
experiments with \textsc{u-dop} for both morphology and phrase structure are possible.

\subsection{About Data-Oriented Parsing}

Data-Oriented Parsing (Scha 1990; Bod \& Scha 1996) is a
computational framework for modeling natural language processing (\textsc{nlp}) and
other hierarchical cognitive phenomena. Its basic assumptions are:

\begin{enumerate}
\item knowledge of language is made up of a corpus of concrete experiences
rather than abstract rules; this concrete experience is stored in
exemplars, pairings of surface forms and their structure.
\item when faced with a new sentence, all fragments of past experiences can be
consulted to analyze the given sentence
\item fragments can be combined using one or more operations which obtain with a
certain (estimated) probability
\end{enumerate}

Two crucial aspects are the representation used to describe the concrete
experiences and the method for ranking the possible analyses. Most research in
Computational Linguistics currently focuses on isolated sentences annotated
with phrase-structures trees; this project will follow the same approach with
the addition of morphological structure. Various methods for selecting the best
parse tree exist for \textsc{dop}; the best performing methods combine a notion of
simplicity (the derivation requiring the least amount of fragments) with
likelihood (estimated probability); e.g., the most likely from the {\em n} shortest
derivations.

It should be noted that Esperanto, as a free word-order language, is more
suitably described using depedency structures. However, given the extent of
previous work on \textsc{dop} with phrase-structure trees, I have opted to assume such
hierarchical representantions instead. This is merely a pragmatically motivated
assumption. 

What makes \textsc{dop} so promising is that if any computational approach to language
can be said to successfully learn a language given enough data (i.e., without
recourse to innate knowledge), \textsc{dop} is bound to be one of them.  This is because
the Data in Data-Oriented Parsing refers to exploiting all of the available
data. Whereas more traditional methods in Computational Linguistics such as
Probabilitistic Context-Free Grammars (PCFG) derive abstract rules from a
treebank, throwing away valuable contextual information, \textsc{dop} retains all
exemplars and their fragments (modulo some potential pruning method
corresponding to memory decay depending on usage and age etc.). This allows for
the recognition of long-range dependencies such as in the construction ``more X
than Y." Also, compared to a PCFG, the statistical independence assumptions of
\textsc{dop} are weaker, because they can be spread over different derivations
resulting in the same parse tree (i.e., the assumptions made by each of the
derivations of the most probable parse are corroborated by its other
derivations). Prescher et al. (2004) observe that \textsc{dop} combines the memory-based
aspects of non-probabilistic machine learning techniques such as k-nearest
neighbor with a probabilistic approach to deal with unseen (novel) exemplars;
thus \textsc{dop} provides a way to deal with the spectrum ranging from stock phrases
that can be memorized by rote to completely novel sentences. The larger the
fragments used in a derivation, the less independence assumptions need to be
made; however, novel sentences can be parsed by backing off to smaller
fragments. Thus, in the limit (as the corpus size approaches infinity) \textsc{dop} does
not make any independence assumptions at all. 

\subsection{Parallels to Data-Oriented Parsing}

A fascinating parallel could be said to exist between \textsc{dop} and the human immune
system:

\begin{quote}
	``Edelman received the Nobel prize in 1972 for his model of the
	recognition processes of the immune system. Recognition of bacteria is
	based on competitive selection in a population of antibodies. This
	process has several intriguing properties (p. 78): 
	
	\renewcommand{\theenumi}{(\arabic{enumi})}
	\renewcommand{\labelenumi}{\theenumi}

	\begin{enumerate} % enumerate with 1) style

	\item There is more than one way to recognize successfully any particular shape;

	\item No two people have identical antibodies;

	\item The system exhibits a form of memory at the cellular level (prior to
	antibody reproduction).

	\end{enumerate}

	Edelman extends this theory to a more general ``science of recognition": 

	By ``recognition," I mean the continual adaptive matching or fitting of
	elements in one physical domain to novelty occurring in elements of
	another, more or less independent physical domain, a matching that
	occurs without prior instruction. [T]here is no explicit information
	transfer between the environment and organisms that causes the
	population to change and increase its fitness. (p. 74)" 
	-- Clancey (1991)
\end{quote}

This general theory that is hinted at here is Edelman's Neural Darwinism,
a theory of competition describing the development of the human brain and
the development of consciousness. The ``species" selected for might be mental
categories, conceptualizations, linguistic exemplars, etc. 

\textsc{dop}'s notion of {\em spurious ambiguities} (different ways of deriving the same
parse tree) accords perfectly with (1).  While \textsc{dop} does not explicitely claim
that ``no two people have identical [exemplars]", it might very well be (which
dramatically changes the scope of \textsc{dop} from a potentially purely linguistic
account modeling a language to a necessarily psychological one modeling an
idiolect); certainly no two individuals will have the exact same corpus. I am
unsure exactly how to interpret (3), but reliance on memory is certainly the
defining trait of \textsc{dop} (as opposed to other formalisms which are typically
biased to computation over memory).

\subsection{DOP and Esperanto}

I attribute the idea of applying Data-Oriented Parsing to Esperanto to
Ken Miner:

\begin{quote}
	``\emph{E\^c se ni disvolvus stokastajn alirojn bazitajn sur Datum-Orientita
	Pritraktado (\textsc{dop}), ankora\v{u} necesus denaskaj parolantoj por validumi
	tiajn modelojn. Kiam temas pri la normala lingvistiko, ne eblas eskapi
	la neceson de denaskaj parolantoj kiel fina kontrolo.}'' -- Miner (2006a)

	Even if we develop stochastic approaches based on Data-Oriented
	Parsing (\textsc{dop}), native speakers would still be necessary for validating
	such models. When we speak of normal linguistics, it is impossible
	to escape the necessity of native speakers as the ultimate arbiters.
\end{quote}

The quote is from a rather gloomy article on the lack of negative evidence
for Esperanto, and the resulting impossibility of doing real linguistics (as
opposed to the parochial ``Esperantology''). Note that ``native speakers''
refers here specifically to speakers who use Esperanto in their day-to-day
life with their peers, not in the broader sense of any language learned from
parents. Native speakers in the latter sense exist but play a marginal role in
the Esperanto movement, native speakers in the former sense do not exist and
would violate the relative neutrality of Esperanto as an international
language. I personally do not think this lack of evidence makes linguistics on
Esperanto problematic, because gramaticality judgements and semantic
intuitions are philosophically problematic no matter how many native speakers
are available to supply them. While it is correct that there can be no
negative evidence about the grammaticality or felicity of an Esperanto
construction, the same goes for writing a poem in English: as long as the poem
is required to be novel and original it needs to be composed with recourse to
some creative ``estimator'' which judges whether a novel combination of words
makes sense; positive evidence from corpora is of little value to this task,
because it is biased to rehashing previously learned constructions, although
it is undoubtedly a precondition as a language model.  Such a creative estimator must prune the
potentially infinite space of low probability events according to a subjective
aesthetic ranking and threshold. Incidentally, it should be noted that
Esperanto has a startingly rich tradition of translated and original poetry,
ranging from its very inception up to the present day.  Poetry has been one
of the driving forces in coining neologisms, because of their affective
connotations. It may be desired to express an antonymic meaning without
evoking its opposite through the presence of its morpheme, compare ``malgaja''
(un-cheerful, sad) and ``trista''; similarly it may be dangerous to refer to
``maldekstra'' (left) in a noisy environment, as opposed to its proposed
neologism ``live.''

The appropriateness of \textsc{dop} for Esperanto should be noted. In contrast with the
earlier a priori, philosophical languages published as completed projects
(Maat 1999), Esperanto was presented in a modest brochure (Zamenhof 1887)
purporting to fully describe its grammar in 16 rules, along with examples of
original and translated prose and poetry, inviting the reader to start
building and using the language by following its examples. That Zamenhof
summarized his language in 16 rules may well have been a nod to the rival
constructed language Volap\"uk (Schleyer 1884), a popular but highly complex
language of bygone days purportedly communicated to its author by God.  The
complexity of Volap\"uk is demonstrated by the fact that its verb paradigm
contains 1584 conjugations, by combining tense, aspect, voice, person, number
and gender, among others. Such features made Volap\"uk difficult to learn and
use, just as the philosophical languages. During the first Esperanto congress
the {\em Fundamento} (Zamenhof 1905) was ratified as the untouchable foundation
of the language\footnote{This document is perusable online at
\url{http://www.akademio-de-esperanto.org/fundamento/index.html}}, containing
the 16 grammar rules, a dictionary with 2600 words and translations in six
languages, and a collection of
exercises; all of these had been published at least a decade earlier and where
already sanctioned through practice. In effect, the {\em Fundamento} can be
considered as the authoritative corpus on Esperanto, to which only new
vocabulary is to be added as needed, provided that it follows Esperanto orthography.
Concerning morphology in particular, Schubert (1993) notes, after referring to
Zamenhof instruction of consulting the supplied dictionary of roots and
affixes:

\begin{quote}
	``Apart from this recipe for deciphering Esperanto texts,
	Zamenhof did not tell the users of his language exactly HOW to
	build complex words. He relied on providing a vast number of models and
	examples" (emphasis in the original)
\end{quote}

Further on, Schubert notes:

\begin{quote}
	``Zamenhof may have intuitively felt the impossibility of describing a
	language exhaustively by means of rules. Such an insight would make his
	thinking very modern indeed. In any case he preferred to give examples
	rather than working out a detailed word grammar."
\end{quote}

This clearly justifies our intention of analyzing Esperanto using an
exemplar-based model, not only pragmatically because of \textsc{dop}'s success, but
historically as well, since it accords with Esperanto's emergence. An
interesting sidenote is that in the years after its publication, Esperanto's
word formation processes appear to have regularized (Schubert 1989), favoring
new coinings such as ``aspekti" (to appear) over semantically opaque Germanisms
such as ``elrigardi" (Wennergren 2005) (literally to look out) in the sense of
to appear (Dutch ``er uitzien", German ``aussehen"), naturally the literal sense
of looking out e.g. a window remains. While such systematization may be reminiscent of
creolization where a pidgin acquires a relatively complex rule system, it should be
noted that Esperanto is neither a pidgin (since it has a grammar) nor a creole; an
argument against the creolization of Esperanto is that creolization
is by definition driven by a newly formed, geographically homogeneous community
of native speakers, which Esperanto certainly does not have. Furthermore, if
Esperanto were to be a pidgin (it is not; cf.  Haitao 2001), it would be one of
an extremely curious sort: a pidgin with an authoritative corpus and a language
academy overseeing its development. As Miner (2008) remarks, the latter is
something which Chomsky could have facetiously remarked, but instead he has claimed
(quite incorrectly) that Esperanto is not a language because it lacks a generative
grammar\footnote{A very debatable implicature is being made that natural
languages do have a generative grammar, with all the assumptions that come with
that.}, putatively because it ``parasitizes'' on other
languages\footnote{This quotation is paraphrased from an interview transcript available at
\url{http://www3.sympatico.ca/mlgr/chomsky.pdf}}; this clearly belies his
ignorance of Esperanto (only its vocabulary is borrowed from European
languages, its grammar is autonomous (Jansen 2007)), as well as being an
obvious non-sequitur (perhaps Chomsky implicitly believes that {\em real}
languages develop {\em de novo} without any interlinguistic interaction to
speak of).

\pagebreak
\section{Practice}
\subsection{Tag set}

The tag set that I will use for the hand-annotated corpora, 
inspired by the Penn-treebank, is as follows:

\begin{itemize}
\item Constituents: VP, PP, NP, N' (constituents that behave like a noun), 
NC (conjunction + NN/N'), NPC (conjunction + NP), VPC (conjunction + VP), 
SC (conjuction + S), S' (if/that + S). 
\item Part-of-speech (simplified version of Penn tagset): NN, VB, PR, JJ, DT, RB, PRP, CC, UH
\item Morphology, open class: N (noun), V (verb), J (adjectival), 
closed class: P (prefix), S (suffix), and auto-generated unique tags for
all grammatical endings and declensions (o, j, n, etc.).
\end{itemize}

The Monato treebank uses a different tag set, based on the 
EspGram\footnote{\url{http://beta.visl.sdu.dk/visl/eo/index.php}} constraint
grammar. The POS tags of the morphology corpus should be adapted to fit those
of the Monato treebank.

Figure \ref{s1} and \ref{s2} shows some annotated example sentences.

%\begin{verbatim}
%(S (NP (NN amiko)) (VP (VB venis)))
%\end{verbatim}

%{i(/phpsyntaxtree/pngtree.php?data=[S [NP [NN amiko]] [VP [VB venis]]])}
%\synttree [S [NP [NN amiko]] [VP [VB venis]]]
\begin{figure}
\centering
\includegraphics[scale=0.5]{eoimg/tree1}
\caption{Translation: a friend came}
\label{s1}
\end{figure}

\begin{comment}
\begin{verbatim}
(S (S (NP (DT la) (N' (JJ venontajn) (N' (JJ apartajn) (NN pecojn)))) 
(VP (NP (PRP mi)) (VP (VBP donas)))) (S' (IN ke) (S (NP (DT la) (NN lernantoj)) 
(VP (VB povu) (VP (VP (VP (VB ripeti) (RB praktike)) (NP (NP (DT la) (NN regulojn)) 
(PP (IN de) (NP (DT l') (N' (NN gramatiko) (JJ internacia)))))) 
(VPC (CC kaj) (VP (VP (VB kompreni) (RB bone)) (NP (NP (NP (DT la) (NN signifon)) 
(NPC (CC kaj) (NP (DT la) (NN uzon)))) (PP (IN de) 
(NP (DT l') (N' (NN sufiksoj) (NC (CC kaj) (NN prefiksoj)))))))))))))
\end{verbatim}
\end{comment}

%{i(/phpsyntaxtree/pngtree.php?fontsize=6\&data=[S [S [NP [DT la] [N' [JJ venontajn] [N' [JJ apartajn] [NN pecojn]]]] [VP [NP [PRP mi]] [VP [VBP donas]]]] [S' [IN ke] [S [NP [DT la] [NN lernantoj]] [VP [VB povu] [VP [VP [VP [VB ripeti] [RB praktike]] [NP [NP [DT la] [NN regulojn]] [PP [IN de] [NP [DT l'] [N' [NN gramatiko] [JJ internacia]]]]]] [VPC [CC kaj] [VP [VP [VB kompreni] [RB bone]] [NP [NP [NP [DT la] [NN signifon]] [NPC [CC kaj] [NP [DT la] [NN uzon]]]] [PP [IN de] [NP [DT l'] [N' [NN sufiksoj] [NC [CC kaj] [NN prefiksoj]]]]]]]]]]]]])97}
%\synttree [S [S [NP [DT la] [N' [JJ venontajn] [N' [JJ apartajn] [NN pecojn]]]] [VP [NP [PRP mi]] [VP [VBP donas]]]] [S' [IN ke] [S [NP [DT la] [NN lernantoj]] [VP [VB povu] [VP [VP [VP [VB ripeti] [RB praktike]] [NP [NP [DT la] [NN regulojn]] [PP [IN de] [NP [DT l'] [N' [NN gramatiko] [JJ internacia]]]]]] [VPC [CC kaj] [VP [VP [VB kompreni] [RB bone]] [NP [NP [NP [DT la] [NN signifon]] [NPC [CC kaj] [NP [DT la] [NN uzon]]]] [PP [IN de] [NP [DT l'] [N' [NN sufiksoj] [NC [CC kaj] [NN prefiksoj]]]]]]]]]]]]]
\begin{figure}
\centering
\includegraphics[width=1.6\textwidth, angle=90]{eoimg/tree2}
%\includegraphics[width=17cm]{eoimg/tree2}
\caption{Translation: the following separate pieces I give so that the students
will be able to rehearse practically the rules of the international grammar and understand properly the meaning and usage of the suffixes and affixes}
\label{s2}
\end{figure}

Some annotated example words are listed in figure \ref{w1}.

\begin{comment}
\begin{verbatim}
(JJ (JJ (V (V (P en) (V konduk)) (V it)) a) j)

(NN (N (J (J (A mal) (J riĉ)) (A eg)) (A ul)) o)

(VB (P al) (VB (V glu) i))
\end{verbatim}
\end{comment}

%{i(/phpsyntaxtree/pngtree.php?data=[JJ [JJ [V [V [P en] [V konduk]] [V it]] a] j])} {i(/phpsyntaxtree/pngtree.php?data=[NN [N [J [J [A mal] [J rich]] [A eg]] [A ul]] o])} {i(/phpsyntaxtree/pngtree.php?data=[VB [P al] [VB [V glu] i]])}
%\synttree [JJ [JJ [V [V [P en] [V konduk]] [V it]] a] j]
%\synttree [NN [N [J [J [A mal] [J rich]] [A eg]] [A ul]] o]
%\synttree [VB [P al] [VB [V glu] i]]
\begin{figure}
\includegraphics[scale=0.5]{eoimg/tree3}
\includegraphics[scale=0.5]{eoimg/tree4}
\includegraphics[scale=0.5]{eoimg/tree5}
\caption{Sample words. Translations: those-that-were-introduced, 
very-poor-person, gluing-to}
\label{w1}
\end{figure}

\subsection{Implementation}

The implementation uses the Goodman (1996) reduction of \textsc{dop} to a PCFG. I have
written my own
implementation\footnote{Full source code available at \url{http://www.github.com/andreasvc/eodop}, including the code dealing with morphology and preprocessing etc.}, using
\href{http://groups.google.com/group/nltk-dev/browse_thread/thread/86ca038723195978/c112b8d171b33d25}{\textsc{nltk}} (Bird et al., 2009). Future work should extend
this implementation to add better estimators such as Backoff \textsc{dop} or \textsc{dop}*.

After producing a PCFG parsing is done using
\href{http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html}{bitpar} 
(Schmid 2004), an efficient bit vector based chart parser.

%guest2@vici.science.uva.nl[9]~/andreas/eodop/ % time python morph.py
%writing grammar
%python morph.py  2121.87s user 32.43s system 99% cpu 36:12.34 total
%guest2@vici.science.uva.nl[9]~/andreas/eodop/ % echo $[2122/60.0]
%35.366666666666667
%
% i.e., reduction takes 35 minutes, using about 4GB

In order to apply the Goodman reduction to an arbitrary treebank, the reduction
has been generalized to deal with arbitrary trees (not just trees in Chomsky
normal form). This is done by translating subtrees of the form 
($A$ $B_1$ ... $B_n$) to rules of the form $A \rightarrow B_1 ... B_n$ with
relative frequency:

\[
\frac{\displaystyle\prod_{m = 1}^n(
\text{$freq(B_m)$ if $B_m$ {\em has an id} else $1$})}{freq(A)}
\] 

\vspace{2em}
In order to fully separate terminals from non-terminals, all terminals are
assigned an unique tag if they don't have one yet.

\begin{comment}
Previously considered possibilities:

\begin{itemize}
\item \href{http://staff.science.uva.nl/~simaan/dopdis/}{dopdis} (C): already
has Goodman reduction and \textsc{dop}*;
\item \href{http://sourceforge.net/projects/lilian/}{lilian} (Java): has
Goodman reduction, no \textsc{dop}*; also has U-DOP.
\item Gideon Borensztajn's \href{http://staff.science.uva.nl/~gideon/sourcecode/DOPParser.tar.gz}{DOPParser} (Java): has Goodman reduction
\end{itemize}
\end{comment}

\subsection{Segmentation}

Before a morphological structure can be assigned to a word, it must be
segmented into morphemes (similar to tokenization before parsing syntax). While
it is claimed that in agglutinative languages in general and in Esperanto in
particular it is ``trivial" to recover the segments that make up a word (e.g.
Schubert 1993), this is a rather informal remark which is not borne out in
practice.  Morpheme boundaries are not marked, and ambiguities may arise due to
overlapping roots. Hana (1998) notes a lexical homonymy rate of 13.6\%.

I have devised a form of ``Data-Oriented Segmentation" to expand the
coverage of segmentation beyond that of the words in the morphology corpus. The
algorithm works as follows:

\begin{enumerate}
\item take the set of segmented words in the corpus by reading off the leaves
of their trees

\item construct a dictionary from positions to the set of morphemes occurring
at that position

\item generate possible words by taking the cartesian product of all morphemes
occurring at position 0 and 1, corresponding to all possible 2-morpheme words
using the available vocabulary of roots.

\item repeat until position $n$ where n is highest number of morphemes in the
treebank to generate all possible words with $n+1$ morphemes.

\end{enumerate}

Unfortunately this algorithm suffers from overgeneration. This should be
remedied by discarding any segmentations contradicting the initial set of
(supervised) segmentations. 

An alternative method of generating segmentations:

\begin{enumerate}
\item take the set of segmented words in the corpus by reading off the leaves
of their trees, store words as tuples of morphemes.

\item construct a dictionary from number of morphemes to words with that number
of morphemes

\item generate possible words with $n$ morphemes for all suitable $n$ by taking
the pointwise cartesian product of all words with $n$ morphemes; i.e.,
\texttt{cartpi(zip(words[n]))}, which coresponds to:

% isn't this awesome . . .
\[ \displaystyle\bigcup_{n=0}^{ max(\{ \, \vert x \vert \, : \: x \, \in \, words\}) }\prod_{\{x \in words \: : \: \vert x\vert \, = \, n\}} x \]

\end{enumerate}

This still overgenerates, though less so (e.g., word class, plural and
accusative endings in the wrong order; it may be necessary to treat endings
separately). A third way would be to use a bigram model and produce every
possible sequence up till a certain length, which avoids such issues.

When none of these approaches are able to segment a word we can resort to
using the the context-free grammar described above, which recognizes any valid
sequence of morphemes. The reason for using this as a last resort is that it
does not distinguish between attested and unattested segmentations, let alone
generalize over attested segmentations, as the previous methods do. The result
will be that potential ambiguities that play no role in naturalistic data will
rear their head.

In the current implementation I use neither the bigram nor the grammar for
segmentation, as I have not yet encountered instances where it would be
make a difference.

\subsection{POS tags for unknown words}

Since part-of-speech tags are transparently marked in Esperanto, it is
possible to assign tags to any open class word. This can be done using a
Finite-State Automaton such as in figure \ref{posfsa}.

The actual automaton is more complicated, firstly because it is
desirable to make it deterministic, secondly it can be made more strict because
inflections may occur only once and in a fixed order.

A deterministic version replaces the ambigious transitions for the vowels with
transitions back to the root state from the accepting states, and transitions
between all the accepting states; for this reason such an automaton is given rather as
a transition table in \ref{posfsa-tab}.

\begin{figure}
\centering
\includegraphics[scale=0.75]{posfsa-crop}
\caption{A sketch for a finite-state automaton for word classification in
Esperanto.}
\label{posfsa}
\end{figure}

\begin{table}
\begin{tabular}{ccc}
from & to & condition \\ \hline
root & rb & (e) \\
root & jj & (a) \\
root & nn & (o) \\
root & vb & ({i,u}) \\
root & root & ([a-z]) \\
vb & vb & ({i,s,u}) \\
rb & rb & ({e,n})  \\
jj & jj & ({a,j,n}) \\
nn & nn & ({o,j,n})
\end{tabular}
\caption{transition table for the deterministic finite-state word-class
automaton. The start state is root, the final states are the open-class tags
\{nn, jj, rb, vb\}. Strictly speaking special states should be created for
accepting at most one declension (\{j,n\}) or conjugation (\{i,u,s\}), but that
is overgeneration which is not as bad as undercoverage.}
\label{posfsa-tab}
\end{table}


\subsection{DOP model composition}

In order to produce a combined morphology-syntax model, it is necessary to be
able to compose a \textsc{dop} model and a treebank. This is defined in the following
manner:

\begin{enumerate}
\item let $M$ be a \textsc{dop} model and $S$ a treebank, where for example $M$ contains
      morphology and $S$ contains phrase structure trees.
\item the composition $M$ o $S$ yields a new \textsc{dop} model by generating a new
      treebank $S'$ based on the trees in the treebank S annotated with
      analyses of words parsed with M (assuming correct segmentation).
\item treebank $S'$ is generated by iterating over the POS tags of the trees in
      $S$ and substituting each POS tag with a tree from $M$.
\item the morphology-syntax model is obtained by instantiating a \textsc{dop} model from
      $S'$.
\end{enumerate}

Note that this procedure assumes that disambiguation of morphology is both
context- and error-free; the most probable parse is used for decorating the
syntax treebank\footnote{Because of technical limitations I have employed the
$n$ best parse trees to approximate the most probable parse.}. This assumption
should be empirically verified. Miner gives the example of a ``fibestejo", which
could be either a ``fi(bestej)o" (a filthy place for animals), or a ``fi(best)ejo"
(a place for filthy animals).  Another example he cites shows that some
ambiguities can be semantically ruled out: an ``eksklubano" is an ex-member of
a club, not a member of an ex-club (because an ex-club cannot have members).

An example of the procedure:

\begin{verbatim}
S := { (S (NP (NN amiko)) (VP (VB venis))) } 
M := { (NN (N amik) o), (VB (V ven) is) }
S o M = { (S (NP (NN (N amik) o)) (VP (VB (V ven) is))) }
\end{verbatim}
S := %{i(/phpsyntaxtree/pngtree.php?data=[S [NP [NN amiko]] [VP [VB venis]]])}
%\synttree[S [NP [NN amiko]] [VP [VB venis]]]
\includegraphics[scale=0.5]{eoimg/tree6}
M := %{i(/phpsyntaxtree/pngtree.php?data=[NN [N amik] o])}  {i(/phpsyntaxtree/pngtree.php?data=[VB [V ven] is])}
%\synttree [NN [N amik] o]
%\synttree [VB [V ven] is]
\includegraphics[scale=0.5]{eoimg/tree7}
\includegraphics[scale=0.5]{eoimg/tree8}
S o M = %{i(/phpsyntaxtree/pngtree.php?data=[S [NP [NN [N amik] o]] [VP [VB [V ven] is]]])}
%\synttree [S [NP [NN [N amik] o]] [VP [VB [V ven] is]]]
\includegraphics[scale=0.5]{eoimg/tree9}

\subsection{Corpora}

Hand-annotated corpora:

\begin{itemize}

\item Morphology: hand annotated list of 290 words, containing all closed class
      words and affixes, and various open class roots and complex derivations.
      Compiled from various more or less naturalistic sources (e.g., Wennergren 2005,
      Miner 2006b).

\item Syntax: hand annotated list of 14 sentences (first paragraph of
      Zamenhof's Dua Libro). Coverage of morphology is 100\% with respect to
      this corpus. This should be extended to cover the whole {\em Fundamento}.

\end{itemize}

Treebanks:

\begin{itemize}

\item morphology: semi-supervised corpus generated from dictionaries (TBD)

\item syntax: Monato treebank (Bick, personal communication, a corpus parsed
      with EspGram (Bick 2007).  Number of sentences: 1995, tokens: 30,397,
      types: 9247. Average sentence length: 15.338. Treebank requires
      preprocessing, a basic filter was applied to prune parse trees whose
      leaves do not agree with the original input sentence (be it because of a
      parse error in the original or an incorrect conversion); also, unique POS
      tags are inserted for punctuation; about 1500 trees remain afterwards.

\end{itemize}

\section{Results}

\subsection{Results on toy corpora}

Using a syntax and morphological corpus that do not contain the word ``ven'as",
but with a morphology model that can derive it from ``don'as" and the past
tense ``ven'is":

\begin{verbatim}
sentence: amiko venas
morphology:
(NN (N amik) o) (p=0.00417101147028)
(VB (V ven) as) (p=0.000334168755221)
syntax:
error Grammar does not cover some of the input words: "'venas'".
morphology + syntax combined:
['amik', 'o', 'ven', 'as']
(S (NP (NN (N amik) o)) (VP (VB (V ven) as))) (p=1.12188584593e-28)
\end{verbatim}

The corpus contains the plural ``prefiksoj," which is inflected to an accusative here:

\begin{verbatim}
sentence: mi donas prefikson
morphology:
(PRP mi) (p=1.0)
(VB (V don) as) (p=0.0350877192982)
(NN (NN (N prefiks) o) n) (p=6.08906783983e-05)
syntax:
error Grammar does not cover some of the input words: "'prefikson'".
morphology + syntax combined:
['mi', 'don', 'as', 'prefiks', 'o', 'n']
(S
  (NP (PRP mi))
  (VP
    (VB (V don) as)
    (NP (NN (NN (N prefiks) o) n)))) (p=9.85999896556e-46)
\end{verbatim}

However, it is perhaps unfair not to assign categories to unknown words. 
In the following results I let a deterministic finite state automaton
assign the right POS tags to unknown words, and use a list of possible
morpheme tags with uniform probabilities to tag unknown morphemes (for words
with a single root the morpheme tagging will default to the POS tag marked
by the ending, which will usually be correct).

Here is a large sentence from later in the ``Dua Libro" (which is, fittingly,
about word formation in Esperanto):

\begin{verbatim}
sentence: Vortoj kunmetitaj estas kreataj per simpla kunligado
	de simplaj vortoj
morphology:
Vortoj (NN (NN (N Vort) (NN_o o)) (NN_j j))
kunmetitaj (JJ (J (V kunmetit) (J_a a)) (JJ_j j))
estas (VB (V est) (VB_as as))
kreataj (JJ (J (V kreat) (J_a a)) (JJ_j j))
per (IN per)
simpla (JJ (J simpl) (JJ_a a))
kunligado (NN (J kunligad) (NN_o o))
de (IN de)
simplaj (JJ (J (V simpl) (J_a a)) (JJ_j j))
vortoj (NN (NN (N vort) (NN_o o)) (NN_j j))
morphology + syntax combined:
['Vort', 'o', 'j', 'kunmetit', 'a', 'j', 'est', 'as', 'kreat', 
 'a', 'j', 'per', 'simpl', 'a', 'kunligad', 'o', 'de', 'simpl',
 'a', 'j', 'vort', 'o', 'j']
(S
  (NP (NN (NN (N Vort) (NN_o o)) (NN_j j)))
  (VP
    (NP (JJ (J (V kunmetit) (J_a a)) (JJ_j j)))
    (VP
      (VB (V est) (VB_as as))
      (VP
        (JJ (J (V kreat) (J_a a)) (JJ_j j))
        (NP
          (NP
            (JJ (V (N per) (V simpl)) (JJ_a a))
            (NN (N kunligad) (NN_o o)))
          (PP
            (IN de)
            (N\'
              (JJ (J (V simpl) (J_a a)) (JJ_j j))
              (NN (NN (N vort) (NN_o o)) (NN_j j)))))))))
\end{verbatim}

See figure \ref{s3} for the tree.

%{i(https://unstable.nl/phpsyntaxtree/pngtree.php?data=[S [NP [NN [NN [N Vort] [NN_o o]] [NN_j j]]] [VP[NP [JJ [J [V kunmetit]  [J_a a]] [JJ_j j]]][VP [VB [V est] [VB_as as]] [VP[JJ [J [V kreat] [J_a a]] [JJ_j j]][NP [NP[JJ [V [N per] [V simpl]] [JJ_a a]][NN [N kunligad] [NN_o o]]] [PP[IN de][N' [JJ [J [V simpl] [J_a a]] [JJ_j j]] [NN [NN [N vort] [NN_o o]] [NN_j j]]]]]]]]])}
%{i(https://unstable.nl/phpsyntaxtree/pngtree.php?data=
%\synttree [S [NP [NN [NN [N Vort] [NN\_o o]] [NN\_j j]]] [VP[NP [JJ [J [V kunmetit]  [J\_a a]] [JJ\_j j]]][VP [VB [V est] [VB\_as as]] [VP[JJ [J [V kreat] [J\_a a]] [JJ\_j j]][NP [NP[JJ [V [N per] [V simpl]] [JJ\_a a]][NN [N kunligad] [NN\_o o]]] [PP[IN de][N' [JJ [J [V simpl] [J\_a a]] [JJ\_j j]] [NN [NN [N vort] [NN\_o o]] [NN\_j j]]]]]]]]]
\begin{figure}
\centering
\includegraphics[scale=0.5,angle=90]{eoimg/tree10}
\caption{Derivation using the interactionist approach. Translation: Derived words are created using simple concatenation of simple words [NB: words means roots here]}
\label{s3}
\end{figure}

There are some mistakes in segmenting (kun-met-it, kre-at, per simpl-a,
kun-lig-ad).  The phrase structure has mistakes as well, e.g. ``vortoj
kunmetitaj" is a constituent, ``per simpla..." should be a PP but this is
overlooked because it got an incorrect POS tag. But given that the syntax
corpus contains only 14 sentences it is perhaps striking that a parse was
produced at all.

The modularist approach yields the following parse tree:

\begin{verbatim}
syntax \& morphology separate:
Vortoj kunmetitaj estas kreataj per simpla kunligado de simplaj 
	vortoj 
(S
  (NP
    (NN (NN (N Vort) (NN_o o)) (NN_j j))
    (JJ (J (V kunmetit) (J_a a)) (JJ_j j)))
  (VP
    (VP
      (VB (V est) (VB_as as))
      (NP (JJ (J (V kreat) (J_a a)) (JJ_j j)) (IN per)))
    (NP
      (NP (JJ (J simpl) (JJ_a a)) (NN (J kunligad) (NN_o o)))
      (PP
        (IN de)
        (N\'
          (JJ (J (V simpl) (J_a a)) (JJ_j j))
          (NN (NN (N vort) (NN_o o)) (NN_j j)))))))
\end{verbatim}

%{i(https://unstable.nl/phpsyntaxtree/pngtree.php?data=[S [NP[NN [NN [N Vort] [NN_o o]] [NN_j j]][JJ [J [V kunmetit] [J_a a]] [JJ_j j]]] [VP[VP [VB [V est] [VB_as as]] [NP [JJ [J [V kreat] [J_a a]] [JJ_j j]] [IN per]]][NP [NP [JJ [J simpl] [JJ_a a]] [NN [J kunligad] [NN_o o]]] [PP[IN de][N' [JJ [J [V simpl] [J_a a]] [JJ_j j]] [NN [NN [N vort] [NN_o o]] [NN_j j]]]]]]])}
%\synttree [S [NP[NN [NN [N Vort] [NN\_o o]] [NN\_j j]][JJ [J [V kunmetit] [J\_a a]] [JJ\_j j]]] [VP[VP [VB [V est] [VB\_as as]] [NP [JJ [J [V kreat] [J\_a a]] [JJ\_j j]] [IN per]]][NP [NP [JJ [J simpl] [JJ\_a a]] [NN [J kunligad] [NN\_o o]]] [PP[IN de][N' [JJ [J [V simpl] [J\_a a]] [JJ\_j j]] [NN [NN [N vort] [NN\_o o]] [NN\_j j]]]]]]]
\begin{figure}
\centering
\includegraphics[scale=0.5,angle=90]{eoimg/tree11}
\caption{Derivation using the modularist approach, same sentence.}
\label{s4}
\end{figure}

The morphology is identical, but syntactically the results are a little
different, e.g., the first noun and adjective are together in an NP. However,
the preposition ``per" appears oddly at the end of an NP, instead of
introducing a PP (in the previous tree it ended up prefixing an NP because the
model cannot distinguish the difference between word and morpheme boundary).

That the finite state automaton is working can be seen from
the following non-sense input:

\begin{verbatim}
sentence: tiadelaradon teluro didelas
morphology:
tiadelaradon (NN (NN (N tiadelarad) (NN_o o)) (NN_n n))
teluro (NN (N telur) (NN_o o))
didelas (VB (V didel) (VB_as as))
morphology + syntax combined:
['tiadelarad', 'o', 'n', 'telur', 'o', 'didel', 'as']
(S
  (NP (NN (NN (N tiadelarad) (NN_o o)) (NN_n n)))
  (VP (NP (NN (N telur) (NN_o o))) (VP (VB (V didel) (VB_as as)))))
syntax \& morphology separate:
(S
  (NP
    (NN (NN (N tiadelarad) (NN_o o)) (NN_n n))
    (NN (N telur) (NN_o o)))
  (VP (VB (V didel) (VB_as as))))
\end{verbatim}

As can be seen, the words and roots receive the correct POS tags, which
additionally is not derived from the default SVO order.
The \textsc{dop} model where morphology is opaque to syntax considers the two nouns
to be a single noun phrase, which could have been trivially excluded by
attending to the morphology (but perhaps an accusative category label would
have been enough).

\begin{comment}
\subsection{Todo}

\item parse bitpar chart output into \textsc{nltk} (currently only n most probable derivations; 
  we need full chart and maybe shortest derivation, \textsc{sl-dop} etc.)
\item use Reta Vortaro / ergane Esperanto dictionary and root lists 
  to induce segmentation / morphology model in a semi-supervised fashion.
\item check morphology coverage against vocabulary of Monato treebank
\item distinguish between morpheme and word boundaries (how?).
  possibly by having a trailing space as part of a morphological analysis 
  (but: this should not block inflection for plurality and accusative (+j and +n respectively).
\item write about Dasgupta (2008) \& is there work on \textsc{dop} + dependencies? (yes, Jansen, Schubert, Bick, etc.!)
  mention DLT as older exemplar model (prior art?).
\item Bitpar should ignore non-ascii characters so that all words get a POS
\item look at DOP* / U-DOP

%finished
%reading the word class guesser...finished
%parameter estimation...finished
%1terminate called after throwing an instance of 'std::bad_alloc'
%  what():  St9bad_alloc
%zsh: abort      bitpar -p -b 1 -u unknownwordsm -w pos.dfsa syntax.pcfg syntax.lex
%guest2@vici.science.uva.nl[9]~/andreas/eodop/ % ls

\item list previous work on EO morphology:
	EspGram: list morphological features; seems to be the affixes and endings and lemma, not segmented?
	Hana: two level morphology, morphemes + features, fully segmented.
	DLT: ?? (Schubert (et al?))
	other work? describe Miner? structuralist representation, hierarchical, unlabeled, implicit (trace?) endings for every morpheme.
	


\subsection{Evaluation}

Tenfold testing of Monato treebank, with and without regard for
morphology.

To be done. Unfortunately parsing using the full monato corpus proved to be
too much for bitpar, as it fails with a memory allocation error before
producing any results. Alternatives would be to sample from the subtrees (i.e.,
sample from the rules containing IDs plus all the rules without IDs), or using
a different parser, one optimised more for memory usage.

\end{comment}

\subsection{Future work}

Unknown words are already handled properly by the \textsc{dfsa} classifier supported by
\texttt{bitpar} (save for proper names and foreign words, which are always
problematic). However, dealing with unknown morphemes or morphemes used with an
unattested tag is more difficult because morphemes are not explicitely marked,
except by association with grammatical endings, which is not conclusive because
the word may be the result of a category transformation (e.g., a nominalization).
To deal with this problem I envision a two level Hidden Markov Model (\textsc{hmm}) for
assigning tags to unknown words and morphemes. The first level will be words,
where the hidden layer consists of their POS tags. The second layer will be the
morphemes of those words, where the hidden layer consists of their morpheme
tags. The \textsc{hmm} would be trained on the same training corpus as the \textsc{dop} model,
and its results should be added to the lexicon of the \textsc{dop} model (i.e., the rules
of the form `POS tag $\rightarrow$ terminal'). It is unclear to me how this
affects the \textsc{dop} probability model, but \texttt{bitpar} will certainly have no
qualms with it as it expects frequencies anyway, so as to be able to do
smoothing.

Another important improvement is to add explicit word boundaries when merging
morphological structures with phrase structures. This can probably be
implemented using a new root node containing the morphological analysis and a
single space as children, but has not been explored yet.

Concerning morphology, it should be noted that e.g., Miner (2006b; and others
cited therein) annotate all morphemes in complex words with their implied
grammatical endings, such that a ``vapor\^sipo'' (steamboat) is analyzed as
``vaporo-\^sipo.''\footnote{Note that both surface forms are technically
correct, but in practice the latter is only used for phonological reasons relating to
prosody etc.} Such an analysis introduces additional ambiguity and departs from
the empirical evidence, but it may be necessary for linguistic reasons.  I have
not considered it because I do not consider it realistic to assume such trace
elements, but there is no evidence against them other than their absence in
most surface forms. On the other hand, the morphological structures that Miner
presents are unlabeled, which makes it harder to generalize over multiple
exemplars as categories are only represented in terminals. The choice to label
morphological constituents with the same labels as individual roots seems
useful and justifiable.

The Distributed Language Translation (\textsc{dlt}, cf., Sadler 1989) project,
which used Esperanto as a pivot language for Machine Translation, went as far
as adding accusative endings inside complex words where implied. This goes
against the grammar of Esperanto, but it is obviously useful for
disambiguation.

\section{Conclusion}

We have described a regular grammar that enumerates the word forms of
Esperanto's lexicon, which can be used to automatically segment word strings.
Using a \textsc{dop} model the resulting sequence of morphemes and tags can be analysed
and assigned a hierarchical structure. The resulting \textsc{dop} model can either be
merged with a syntactic treebank into a combined \textsc{dop} model, or mapped to the
leaves of the parse trees produced by a syntactic model, to obtain tree
structures with both phrasal and morphological constituents.

We described an implementation using \textsc{nltk} of the Goodman reduction that is
generalized to arbitrary trees, which outputs a grammar that can be parsed by
the efficient chart parser Bitpar. Using a list of open class tags and a
finite state automaton we can assign tags to unknown words and
morphemes.

The resulting system has been applied to a small corpus of morphology and
syntax, hinting at the advantage of merging morphology and syntax treebanks
before constructing a \textsc{dop} model. Evaluation with a larger syntactic treebank,
as well as the induction of morphology tags from dictionaries remains to be
done. However, the groundwork for such an enterprise has been laid, as well as
for addressing the research questions. It seems likely that morphology
will be crucial for parsing syntax, especially when the morphology in question
is rich and highly structured.

\begin{center}
$\infty$
\end{center}

%\end{multicols}

\section{References}

\renewcommand*\descriptionlabel[1]{\hspace\labelsep\normalfont#1}

\begin{description*}
\item[Bick,] Eckhard (2007), ``Tagging and Parsing an Artificial Language: an
annotated web-corpus of Esperanto,'' in: {\em Proceedings of Corpus
Linguistics}, Birmingham, UK. \\
\url{http://beta.visl.sdu.dk/pdf/CorpusLinguistics2007_esp.pdf}

\item[Bird,] Steven, Edward Loper \& Ewan Klein (2009).
    ``Natural Language Processing with Python.''  O'Reilly Media Inc.

\item[Bod,] Rens \& Scha, Remko (1996) ``Data-Oriented Language Processing: an
overview.'' Research reports, Institute for Logic, Language and Computation,
University of Amsterdam. \\
\url{http://dare.uva.nl/document/1144}

\item[Clancey,] W.J. (1991), ``The biology of consciousness: Comparative review
of Israel Rosenfield, The Strange, Familiar, and Forgotten: An anatomy of
Consciousness and Gerald M. Edelman, Bright Air, Brilliant Fire: On the Matter
of the Mind,'' {\em Artificial Intelligence} vol. 60, pp. 313--356

\item[Gobbo,] Federico (2009), ``Adpositional Grammars: a multilingual grammar
formalism for NLP,'' PhD dissertation, Universita degli Studi dell'Insubria.

\item[Goodman,] Joshua (1996), ``Efficient Algorithms for Parsing the DOP
Model''. {\em Proceedings Empirical Methods in Natural Language Processing},
pp. 143-152. \\ \url{http://acl.ldc.upenn.edu/W/W96/W96-0214.pdf}

\item[Jackendoff,] Ray (2003), ``Précis of Foundations of Language: Brain, Meaning,
Grammar, Evolution,'' Behavioral and Brain Sciences (2003), 26:6:651-665
Cambridge University Press.

\item[Jansen,] W. (2007). ``Woordvolgorde in het Esperanto: normen, taalgebruik en
universalia" (Word-order in Esperanto: norms, usage and universals). PhD
thesis, LOT Utrecht.

\item[Jurafsky,] D. \& Martin, J.H. (2000), ``Speech \& Language Processing An
introduction to natural language processing, computational linguistics, and
speech recognition,'' Pearson Education.

\item[Hana,] Ji\~ri, ``Two-level morphology of Esperanto,'' MSc thesis, Charles
University Prague, Faculty of Mathematics and Physics.
\url{http://www.ling.ohio-state.edu/~hana/esr/thesis.html}

\item[Haitao,] Liu (2001), ``Creoles, Pidgins, and Planned Languages.'' Interface.
Journal of Applied Linguistics / Tijdschrift voor Toegepaste Linguïstiek 15 [2]. pp. 121--177.

\item[Kalocsay,] K\'alm\'an \& Waringhien, Gaston (1980), Plena Analiza Gramatiko de
Esperanto (Complete, analyzed Grammar of Esperanto), Rotterdam, Universala
Esperanto-Asocio.

\item[Maat,] Jaap (1999), ``Philosophical Languages in the Seventeenth Century:
Dalgarno, Wilkins, Leibniz,'' Amsterdam, Institute for Logic, Language and
Computation.

\item[MacWhinney,] B. (1987), ``Mechanisms of Language Acquisition,'' Lawrence Erlbaum Associates, NJ.

\item[Miner,] Ken (2006a), ``Tranchitaj frazoj kaj la probleme pri negative
evidento'' (Cut phrases and the problem of negative evidence). March 2006. \\
\url{http://www.sunflower.com/~miner/NEGATIVA_package/negativa.html}

\item[Miner,] Ken (2006b), ``Rimarkoj pri `En la komenco estas la vorto' de Geraldo
Mattos (fina versio),'' (Comments on `In the beginning was the word' by Geraldo
Mattos (final version)). \\
\url{http://www.sunflower.com/~miner/EKVO_package/ekvo.html}

\item[Miner,] Ken (2008), ``La neebleco de priesperanto lingvoscienco,'' (The
impossibility of Esperanto linguistics). October 2008. \\
\url{http://www.sunflower.com/~miner/LINGVISTIKO_package/lingvistiko.html} \\
Also published in ``La arto labori kune: festlibro por Humphrey Tonkin'' (The
art of working together: Festschrift for Humphrey Tonkin). Roterdam, Universala
Esperanto Asocio, January 2010

\item[Pinker,] S. (1994). The language instinct: How the mind creates language. New York: W. Morrow.

\item[Prescher,] D., Scha, R., Sima`an, K., Zollmann, A., (2004) ``On the statistical
consistency of DOP estimators.'' In {\em Proceedings of the 14th Meeting of
Computational Linguistics in the Netherlands}, Antwerp, Belgium.

\item[Sadler,]  Victor (1989), ``Working with analogical semantics: disambiguation techniques in DLT,''
	Foris Publications, Dordrecht, The Netherlands.

\item[Scha,] Remko (1990), ``Taaltheorie en Taaltechnologie; Competence en
Performance'' (Language theory and language technology: Competence and
Performance), in Q.A.M. de kort and G.L.J. Leerdam (eds.), {\em
Computertoepassingen in de Neerlandistiek} pp. 7-22, Almere: Landelijke
Vereniging van Neerlandici (LVVN-jaarboek). English translation
\url{http://www.hum.uva.nl/computerlinguistiek/scha/IAAA/rs/cv.html}

\item[Schleyer,] Johan Martin (1884), ``Volap\"uk. Grammatik der Universalsprache f\"ur
alle gebildete Erdbewohner,'' \"Uberlingen am Bodensee: Buchdruckerei August
Feyel, Buchhandlung Aug. Schoy. Third edition.

\item[Schmid,] Helmut (2004), ``Efficient Parsing of Highly Ambiguous Context-Free
Grammars with Bit Vectors,'' {\em Proceedings of the 20th International Conference
on Computational Linguistics} (COLING 2004), Geneva, Switzerland.
\url{http://www.ims.uni-stuttgart.de/www/projekte/gramotron/PAPERS/COLING04/BitPar.pdf}

\item[Schmid,] Helmut, Arne Fitschen and Ulrich Heidi (2004), SMOR: A German
Computational Morphology Covering Derivation, Composition, and Inflection,
Proceedings of the IVth International Conference on Language Resources and
Evaluation (LREC 2004), p. 1263-1266, Lisbon, Portugal.
\url{http://www.ims.uni-stuttgart.de/www/projekte/gramotron/PAPERS/LREC04/smor.pdf}

\item[Schubert,] Klaus, 1989. ``An unplanned development in planned languages",
en Klaus Schubert, red., Interlinguistics: Aspects of the Science of Planned
Languages [ = Trends in Linguistics: Studies and Monographs 42], Mouton de
Gruyter.

\item[Schubert,] Klaus (1993), ``Semantic compositionality: Esperanto
word-formation for language technology.'' {\em Linguistics} 31: 311-365.

\item[Tsarfaty,] Reut (2010). ``Relational-Realizational Parsing,'' PhD thesis,
Institute for Language, Logic and Computation (ILLC), University of Amsterdam.

\item[Wells,] John (1989), ``Lingvistikaj aspektoj de Esperanto,'' Universala
Esperanto Asocio, Rotterdam. Second edition.

\item[Wennergren,] Bertilo (2005), ``Plena Manlibro de Esperanta Gramatiko,''
(Complete handbook of Esperanto Grammar), version 13.0, 14th of April 2005.
Available online at \url{http://bertilow.com/pmeg/}.

\item[Zamenhof,] Dr. L. L. (1887/1968), ``Internationale Sprache. Vorrede und
Vollst\"andiges Lehrbuch,'' Warschau, photographic reprint from 1968
(Saarbr\"ucken: Artur E. Illtis). German translation of the original Russian
brochure.

\item[Zamenhof,] Dr. L. L. (1905/1963), ``Fundamento de Esperanto.'' Ninth
edition with Introduction, Notes and Linguistics comments, edited by Dr. A.
Albault (Esperantaj Francaj Eldonoj: Marmande, 1963).

\item[Zollmann,] Andreas \& Sima'an, Khalil (2005), ``A Consistent and Efficient
Estimator for DOP.''  {\em Journal of Automota Languages and Combinatorics} vol.
10, pp. 367.  \url{http://staff.science.uva.nl/~simaan/D-Papers/JALCsubmit.pdf}

\end{description*}

\end{document}


\subsection{Needed references}

everything seems to be there.

\subsection{Possible references }

Dasgputa, Probal (2008), ``Interlexical studies: a cognitive approach,'' talk
delivered on 18th of April 2008, Amsterdam Centre for Language and
Communication.

DLT: Distributed Language Translation project.

(from Miner 2006a)

Sakaguchi, Alicja, 1996. Die Dichotomie "künstlich" vs. "natürlich" und das historische Phänomen einer funktionierenden Plansprache. Language Problems and Language Planning 20:1.

Gledhill, Christopher, 2000. The Grammar of Esperanto: A Corpus-Based Description. Lincom Europa.

Grimley-Evans, Edmundo, 1997. "Vortfarado", (Word derivation) La Brita Esperantisto, marto-aprilo 1997, pp. 57-59.


\section{Appendix}

GRAMMAR

A) THE ALPHABET
Aa,
a as in
"last"	Bb,
b as in
"be"	Cc,
ts as in
"wits"	Ĉĉ,
ch as in
"church"	Dd,
d as in
"do"	Ee,
a as in
"make"	Ff,
f as in
"fly"
Gg,
g as in
"gun"	Ĝĝ,
j as in
"join"	Hh,
h as in
"half"	Ĥĥ,
strongly
aspirated
h, "ch"
in "loch"
(scotch)	Ii,
i as in
"marine"	Jj,
y as in
"yoke"	Ĵĵ,
z as in
"azure"
Kk,
k as in
"key"	Ll,
l as in
"line"	Mm,
m as in
"make"	Nn,
n as in
"now"	Oo,
o as in
"not"	Pp,
p as in
"pair"	Rr,
r as in
"rare"
Ss,
s as in
"see"	Ŝŝ,
sh as in
"show"	Tt,
t as in
"tea"	Uu,
u as in
"bull"	Ŭŭ,
u as in
"mount"
(used in
diphtongs)	Vv,
v as in
"very"	Zz,
z as in
"zeal"
Remark. ― If it be found impraticable to print works with the diacritical signs (^,˘), the letter h may be substituted for the sign (^), and the sign (˘), may be altogether omitted.
B) PARTS OF SPEECH
1. There is no indefinite, and only one definite, article, la, for all genders, numbers, and cases.
2. Substantives are formed by adding o to the root. For the plural, the letter j must be added to the singular. There are two cases: the nominative and the objective (accusative). The root with the added o is the nominative, the objective adds an n after the o. Other cases are formed by prepositions; thus, the possessive (genitive) by de, „of”; the dative by al, „to”, the instrumental (ablative) by kun, „with”, or other preposition as the sense demands. E. g. root patr, „father”; la patr'o, „the father”; la patr'o'n, „the father” (objective), de la patr'o, „of the father”; al la patr'o, „to the father”; kun la patr'o, „with the father”; la patr'o'j, „the fathers”; la patr'o'j'n, „the fathers” (obj.), por la patr'o'j, „for the fathers”.
3. Adjectives are formed by adding a to the root. The numbers and cases are the same as in substantives. The comparative degree is formed by prefixing pli (more); the superlative by plej (most). The word „than” is rendered by ol, e. g. pli blanka ol neĝo, „whiter than snow”.
4. The cardinal numerals do not change their forms for the different cases. They are: unu (1), du (2), tri (3), kvar (4), kvin (5), ses (6), sep (7), ok (8), naŭ (9), dek (10), cent (100), mil (1000). The tens and hundreds are formed by simple junction of the numerals, e. g. 533 = kvin'cent tri'dek tri. Ordinals are formed by adding the adjectival a to the cardinals, e. g. unu'a, „first”; du'a, „second”, etc. Multiplicatives (as „threefold”, „fourfold”, etc.) add obl, e. g. tri'obl'a, „threefold”. Fractionals add on, as du'on'o, „a half”; kvar'on'o, „a quarter”. Collective numerals add op, as kvar'op'e, „four together”. Distributive prefix po, e. g., po kvin, „five apiece”. Adverbials take e, e. g., unu'e, „firstly”, etc.
5. The personal pronouns are: mi, „I”; vi, „thou”, „you”; li, „he”; ŝi, „she”; ĝi, „it”; si, „self”; ni, „we”; ili, „they”; oni, „one”, „people”, (French „on”). Possessive pronouns are formed by suffixing to the required personal, the adjectival termination. The declension of the pronouns is identical with that of substantives. E. g. mi, „I”; mi'n, „me” (obj.); mi'a, „my”, „mine”.
6. The verb does not change its form for numbers or persons, e. g. mi far'as, „I do”; la patr'o far'as, „the father does”; ili far'as, „they do”.
Forms of the Verb:
a) The present tense ends in as, e. g. mi far'as, „I do”.
b) The past tense ends in is, e. g. li far'is, „he did”.
c) The future tense ends in os, e. g. ili far'os, „they will do”.
ĉ) The subjunctive mood ends in us, e. g. ŝi far'us, „she may do”.
d) The imperative mood ends in u, e. g. ni far'u, „let us do”.
e) The infinitive mood ends in i, e. g. fari, „to do”.
There are two forms of the participle in the international language, the changeable or adjectival, and the unchangeable or adverbial.
f) The present participle active ends in ant, e. g. far'ant'a, „he who is doing”; far'ant'e, „doing”.
g) The past participle active ends in int, e. g. far'int'a, „he who has done”; far'int'e, „having done”.
ĝ) The future participle active ends in ont, e. g. far'ont'a, „he who will do”; far'ont'e, „about to do”.
h) The present participle passive ends in at, e. g. far'at'e, „being done”.
ĥ) The past participle passive ends in it, e. g. far'it'a, „that which has been done”; far'it'e, „having been done”.
i) The future participle passive ends in ot, e. g. far'ot'a, „that which will be done”; far'ot'e, „about to be done”.
All forms of the passive are rendered by the respective forms of the verb est (to be) and the participle passive of the required verb; the preposition used is de, „by”. E. g. ŝi est'as am'at'a de ĉiu'j, „she is loved by every one”.
7. Adverbs are formed by adding e to the root. The degrees of comparison are the same as in adjectives, e. g., mi'a frat'o kant'as pli bon'e ol mi, „my brother sings better than I”.
8. All prepositions govern the nominative case.
C) GENERAL RULES
9. Every word is to be read exactly as written, there are no silent letters.
10. The accent falls on the last syllable but one, (penultimate).
11. Compound words are formed by the simple junction of roots, (the principal word standing last), which are written as a single word, but, in elementary works, separated by a small line ('). Grammatical terminations are considered as independent words. E. g. vapor'ŝip'o, „steamboat” is composed of the roots vapor, „steam”, and ŝip, „a boat”, with the substantival termination o.
12. If there be one negative in a clause, a second is not admissible.
13. In phrases answering the question „where?” (meaning direction), the words take the termination of the objective case; e. g. kie'n vi ir'as?„where are you going?”; dom'o'n, „home”; London'o'n, „to London”, etc.
14. Every preposition in the international language has a definite fixed meaning. If it be necessary to employ some preposition, and it is not quite evident from the sense which it should be, the word je is used, which has no definite meaning; for example, ĝoj'i je tio, „to rejoice over it”; rid'i je tio, „to laugh at it”; enu'o je la patr'uj'o, „a longing for one’s fatherland”. In every language different prepositions, sanctioned by usage, are employed in these dubious cases, in the international language, one word, je, suffices for all. Instead of je, the objective without a preposition may be used, when no confusion is to be feared.
15. The so-called „foreign” words, i. e. words which the greater number of languages have derived from the same source, undergo no change in the international language, beyond conforming to its system of orthography. ― Such is the rule with regard to primary words, derivatives are better formed (from the primary word) according to the rules of the international grammar, e. g. teatr'o, „theatre”, but teatr'a, „theatrical”, (not teatrical'a), etc.
16. The a of the article, and final o of substantives, may be sometimes dropped euphoniae gratia, e. g. de l’ mond'o for de la mond'o; Ŝiller’ for Ŝiller'o; in such cases an apostrophe should be substituted for the discarded vowel.
